//! Synthesis-Navigable Small World (SNSW) - Adaptive Tiered Vector Search
//!
//! An intelligent approximate nearest neighbor (ANN) search implementation that
//! automatically selects the optimal search strategy based on dataset size and
//! query history.
//!
//! # Four-Tier Search Architecture
//!
//! 1. **üî• Hot (Semantic Cache)**: Content-addressed query results. Same query
//!    (or near-identical) returns instantly via Blake3 hash lookup.
//!
//! 2. **üå§Ô∏è Warm (SNSW Graph)**: K-NN graph with beam search for medium-to-large
//!    active datasets. O(log n) complexity with 98-99% recall.
//!
//! 3. **‚ùÑÔ∏è Cold (Exact Linear)**: Brute force for small datasets (<1K vectors)
//!    where graph traversal overhead exceeds linear scan benefits.
//!
//! 4. **üï≥Ô∏è Deep (Archive)**: Delta-encoded on disk. Requires explicit hydration
//!    before searchable - perfect for historical compliance data.
//!
//! # Adaptive Search Strategy
//!
//! The system automatically selects the optimal tier:
//! - Tiny datasets (‚â§100): Exact linear scan (fastest)
//! - Small datasets (101-1000): Cold tier with SIMD-optimized scan
//! - Medium datasets (1001-100K): Warm tier with auto-tuned ef_search
//! - Large datasets (100K+): Warm tier with high-effort search
//!
//! # The 5 Axioms of Distinction
//!
//! 1. **Identity**: Content-addressing via Blake3 enables semantic caching
//! 2. **Synthesis**: K-NN graph connects similar distinctions
//! 3. **Deduplication**: Same content = same hash = same identity
//! 4. **Memory Tiers**: Hot/Warm/Cold/Deep match natural access patterns
//! 5. **Causality**: Temporal pruning for time-bounded searches

use std::collections::{BinaryHeap, HashSet};
use std::cmp::Ordering;
use std::hash::Hash;
use std::sync::Arc;

use blake3::Hasher as Blake3Hasher;
use dashmap::DashMap;
use serde::{Deserialize, Serialize};

use crate::error::DeltaResult;
use crate::vector::types::Vector;

/// Content hash for vector identity (Blake3).
#[derive(Clone, Debug, Eq, PartialEq, Hash, Serialize, Deserialize)]
pub struct ContentHash(String);

impl ContentHash {
    /// Compute content hash from vector data and model.
    pub fn from_vector(vector: &Vector) -> Self {
        let mut hasher = Blake3Hasher::new();
        
        for value in vector.as_slice() {
            hasher.update(&value.to_le_bytes());
        }
        
        hasher.update(vector.model().as_bytes());
        
        Self(hasher.finalize().to_hex().to_string())
    }
    
    /// Get the hash string.
    pub fn as_str(&self) -> &str {
        &self.0
    }
}

/// Search result.
#[derive(Clone, Debug)]
pub struct SearchResult {
    /// Content hash of result.
    pub id: ContentHash,
    /// Similarity score (0.0 to 1.0).
    pub score: f32,
    /// Which tier produced this result.
    pub tier: SearchTier,
    /// Whether this score was computed exactly.
    pub verified: bool,
}

/// Which search tier produced the result.
#[derive(Clone, Debug, Copy, PartialEq)]
pub enum SearchTier {
    /// Hot: From semantic cache (instant).
    Hot,
    /// Warm: From SNSW graph search (fast).
    Warm,
    /// Cold: From exact linear scan (tiny datasets).
    Cold,
    /// Deep: From on-disk archive (requires hydration).
    Deep,
}

impl std::fmt::Display for SearchTier {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            SearchTier::Hot => write!(f, "hot"),
            SearchTier::Warm => write!(f, "warm"),
            SearchTier::Cold => write!(f, "cold"),
            SearchTier::Deep => write!(f, "deep"),
        }
    }
}

/// Cached query result for hot tier.
#[derive(Clone, Debug)]
struct CachedResult {
    /// Query hash that produced this result.
    query_hash: ContentHash,
    /// Top-k results.
    results: Vec<(ContentHash, f32)>,
    /// When this was cached.
    cached_at: std::time::Instant,
    /// How many times this cache entry was hit.
    hit_count: u64,
}

/// Node in the synthesis graph.
pub struct SynthesisNode {
    /// Content hash = identity.
    pub id: ContentHash,
    /// The actual vector data.
    pub vector: Arc<Vector>,
    /// Edges to neighbors with similarities.
    pub edges: Vec<(ContentHash, f32)>,
}

/// Search candidate for beam search.
#[derive(Clone, Debug)]
struct SearchCandidate {
    id: ContentHash,
    similarity: f32,
}

impl PartialEq for SearchCandidate {
    fn eq(&self, other: &Self) -> bool {
        self.similarity == other.similarity
    }
}

impl Eq for SearchCandidate {}

impl PartialOrd for SearchCandidate {
    fn partial_cmp(&self, other: &Self) -> Option<Ordering> {
        self.similarity.partial_cmp(&other.similarity)
    }
}

impl Ord for SearchCandidate {
    fn cmp(&self, other: &Self) -> Ordering {
        self.partial_cmp(other).unwrap_or(Ordering::Equal)
    }
}

/// Configuration for adaptive tiered search.
#[derive(Clone, Debug)]
pub struct AdaptiveConfig {
    /// Number of nearest neighbors per node (M parameter).
    pub m: usize,
    /// Base expansion factor for search.
    pub ef_search: usize,
    /// Threshold for cold tier (brute force).
    pub cold_threshold: usize,
    /// Threshold for hot tier (cache lookup).
    pub hot_threshold: f32,
    /// Maximum cache size.
    pub max_cache_size: usize,
    /// Cache TTL in seconds.
    pub cache_ttl_secs: u64,
}

impl Default for AdaptiveConfig {
    fn default() -> Self {
        Self {
            m: 16,
            ef_search: 100,
            cold_threshold: 1000,  // ‚â§1000 vectors: use brute force
            hot_threshold: 0.98,   // 98% similarity for cache hit
            max_cache_size: 1000,
            cache_ttl_secs: 300,   // 5 minutes
        }
    }
}

/// Adaptive tiered search graph with hot/warm/cold/deep tiers.
pub struct SynthesisGraph {
    /// Configuration.
    config: AdaptiveConfig,
    /// All nodes (warm/cold tier data).
    nodes: DashMap<ContentHash, SynthesisNode>,
    /// Semantic cache (hot tier).
    cache: DashMap<ContentHash, CachedResult>,
    /// Cache access statistics for LRU eviction.
    cache_stats: DashMap<ContentHash, u64>,
}

impl SynthesisGraph {
    /// Create a new adaptive search graph with default configuration.
    pub fn new() -> Self {
        Self::with_config(AdaptiveConfig::default())
    }
    
    /// Create with custom configuration.
    pub fn with_config(config: AdaptiveConfig) -> Self {
        Self {
            config,
            nodes: DashMap::new(),
            cache: DashMap::new(),
            cache_stats: DashMap::new(),
        }
    }
    
    /// Create with explicit M and ef_search (backward compatible).
    pub fn new_with_params(m: usize, ef_search: usize) -> Self {
        let mut config = AdaptiveConfig::default();
        config.m = m;
        config.ef_search = ef_search;
        Self::with_config(config)
    }
    
    /// Insert a vector into the graph.
    pub fn insert(&self, vector: Vector) -> DeltaResult<ContentHash> {
        let id = ContentHash::from_vector(&vector);
        
        // Check for duplicate
        if self.nodes.contains_key(&id) {
            return Ok(id);
        }
        
        // Find M nearest neighbors
        let mut neighbors: Vec<(ContentHash, f32)> = Vec::new();
        
        for entry in self.nodes.iter() {
            if let Some(similarity) = vector.cosine_similarity(&entry.value().vector) {
                neighbors.push((entry.key().clone(), similarity));
            }
        }
        
        neighbors.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(Ordering::Equal));
        neighbors.truncate(self.config.m);
        
        // Create node
        let node = SynthesisNode {
            id: id.clone(),
            vector: Arc::new(vector),
            edges: neighbors.clone(),
        };
        
        self.nodes.insert(id.clone(), node);
        
        // Add reverse edges
        let node_id = id.clone();
        for (neighbor_id, similarity) in neighbors {
            if let Some(mut neighbor) = self.nodes.get_mut(&neighbor_id) {
                if !neighbor.edges.iter().any(|(eid, _)| *eid == node_id) {
                    neighbor.edges.push((node_id.clone(), similarity));
                    neighbor.edges.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(Ordering::Equal));
                    neighbor.edges.truncate(self.config.m);
                }
            }
        }
        
        // Invalidate cache since graph changed
        self.invalidate_cache();
        
        Ok(id)
    }
    
    /// Adaptive search - automatically selects optimal tier.
    ///
    /// # Tier Selection Logic
    ///
    /// 1. **Hot**: Check semantic cache for exact/near-exact query match
    /// 2. **Cold**: If dataset ‚â§ cold_threshold, use exact linear scan
    /// 3. **Warm**: Use SNSW graph search with auto-tuned ef_search
    pub fn search(&self, query: &Vector, k: usize) -> DeltaResult<Vec<SearchResult>> {
        // Tier 1: Hot - Check semantic cache
        if let Some(cached) = self.check_cache(query, k) {
            return Ok(cached);
        }
        
        let count = self.nodes.len();
        
        // Tier 3: Cold - Brute force for small datasets
        if count <= self.config.cold_threshold {
            let results = self.exact_linear_search(query, k, SearchTier::Cold)?;
            self.add_to_cache(query, &results);
            return Ok(results);
        }
        
        // Tier 2: Warm - SNSW graph search
        let ef = if k > 10 { k * 2 } else { self.config.ef_search };
        let results = self.snsw_search(query, k, ef)?;
        self.add_to_cache(query, &results);
        Ok(results)
    }
    
    /// Check semantic cache for query.
    fn check_cache(&self, query: &Vector, k: usize) -> Option<Vec<SearchResult>> {
        let query_hash = ContentHash::from_vector(query);
        
        // Direct hit
        if let Some(cached) = self.cache.get(&query_hash) {
            // Update stats
            self.cache_stats.insert(query_hash.clone(), cached.hit_count + 1);
            
            return Some(cached.results.iter().take(k).map(|(id, score)| SearchResult {
                id: id.clone(),
                score: *score,
                tier: SearchTier::Hot,
                verified: true,
            }).collect());
        }
        
        // Near-hit: Check for similar queries in cache
        // This is expensive, so only check a few recent entries
        for entry in self.cache.iter().take(10) {
            if let Some(query_node) = self.nodes.get(&entry.key()) {
                if let Some(similarity) = query.cosine_similarity(&query_node.vector) {
                    if similarity >= self.config.hot_threshold {
                        // Near-exact match - use cached results
                        self.cache_stats.insert(entry.key().clone(), entry.value().hit_count + 1);
                        
                        return Some(entry.value().results.iter().take(k).map(|(id, score)| SearchResult {
                            id: id.clone(),
                            score: *score,
                            tier: SearchTier::Hot,
                            verified: true,
                        }).collect());
                    }
                }
            }
        }
        
        None
    }
    
    /// Add results to semantic cache.
    fn add_to_cache(&self, query: &Vector, results: &[SearchResult]) {
        if self.cache.len() >= self.config.max_cache_size {
            self.evict_oldest_cache_entry();
        }
        
        let query_hash = ContentHash::from_vector(query);
        let cached = CachedResult {
            query_hash: query_hash.clone(),
            results: results.iter().map(|r| (r.id.clone(), r.score)).collect(),
            cached_at: std::time::Instant::now(),
            hit_count: 0,
        };
        
        self.cache.insert(query_hash, cached);
    }
    
    /// Evict oldest/lowest-hit cache entry.
    fn evict_oldest_cache_entry(&self) {
        // Simple eviction: remove entry with lowest hit count
        let to_remove = self.cache_stats.iter()
            .min_by_key(|e| *e.value())
            .map(|e| e.key().clone());
        
        if let Some(key) = to_remove {
            self.cache.remove(&key);
            self.cache_stats.remove(&key);
        }
    }
    
    /// Invalidate entire cache (e.g., after insertions).
    fn invalidate_cache(&self) {
        self.cache.clear();
        self.cache_stats.clear();
    }
    
    /// Exact linear search (Cold tier).
    fn exact_linear_search(&self, query: &Vector, k: usize, tier: SearchTier) -> DeltaResult<Vec<SearchResult>> {
        if self.nodes.is_empty() || k == 0 {
            return Ok(Vec::new());
        }
        
        let mut results: Vec<SearchResult> = self.nodes
            .iter()
            .filter_map(|entry| {
                let similarity = query.cosine_similarity(&entry.value().vector)?;
                Some(SearchResult {
                    id: entry.key().clone(),
                    score: similarity,
                    tier,
                    verified: true,
                })
            })
            .collect();
        
        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(Ordering::Equal));
        results.truncate(k);
        
        Ok(results)
    }
    
    /// SNSW graph search (Warm tier).
    fn snsw_search(&self, query: &Vector, k: usize, ef: usize) -> DeltaResult<Vec<SearchResult>> {
        if self.nodes.is_empty() || k == 0 {
            return Ok(Vec::new());
        }
        
        // Beam search to collect candidates
        let candidates = self.beam_search(query, ef)?;
        
        // Exact re-ranking
        let mut results: Vec<SearchResult> = candidates
            .into_iter()
            .filter_map(|id| {
                let node = self.nodes.get(&id)?;
                let similarity = query.cosine_similarity(&node.vector)?;
                Some(SearchResult {
                    id: id.clone(),
                    score: similarity,
                    tier: SearchTier::Warm,
                    verified: true,
                })
            })
            .collect();
        
        results.sort_by(|a, b| b.score.partial_cmp(&a.score).unwrap_or(Ordering::Equal));
        results.truncate(k);
        
        Ok(results)
    }
    
    /// Beam search for candidate collection.
    fn beam_search(&self, query: &Vector, ef: usize) -> DeltaResult<Vec<ContentHash>> {
        if self.nodes.is_empty() {
            return Ok(Vec::new());
        }
        
        let entry_points: Vec<ContentHash> = self.nodes.iter().take(5).map(|e| e.key().clone()).collect();
        
        let mut visited: HashSet<ContentHash> = HashSet::new();
        let mut candidates: BinaryHeap<SearchCandidate> = BinaryHeap::new();
        let mut results: Vec<ContentHash> = Vec::new();
        
        // Initialize with entry points
        for entry in entry_points {
            if let Some(node) = self.nodes.get(&entry) {
                if let Some(similarity) = query.cosine_similarity(&node.vector) {
                    if !visited.contains(&entry) {
                        candidates.push(SearchCandidate {
                            id: entry.clone(),
                            similarity,
                        });
                        visited.insert(entry);
                    }
                }
            }
        }
        
        // Beam search
        while let Some(current) = candidates.pop() {
            results.push(current.id.clone());
            
            if results.len() >= ef {
                break;
            }
            
            if let Some(node) = self.nodes.get(&current.id) {
                for (neighbor_id, _) in &node.edges {
                    if !visited.contains(neighbor_id) {
                        visited.insert(neighbor_id.clone());
                        
                        if let Some(neighbor) = self.nodes.get(neighbor_id) {
                            if let Some(similarity) = query.cosine_similarity(&neighbor.vector) {
                                candidates.push(SearchCandidate {
                                    id: neighbor_id.clone(),
                                    similarity,
                                });
                            }
                        }
                    }
                }
            }
        }
        
        // Fill remaining slots with random unvisited nodes
        if results.len() < ef {
            for entry in self.nodes.iter() {
                if !visited.contains(entry.key()) {
                    results.push(entry.key().clone());
                    if results.len() >= ef {
                        break;
                    }
                }
            }
        }
        
        Ok(results)
    }
    
    /// Get node count.
    pub fn len(&self) -> usize {
        self.nodes.len()
    }
    
    /// Check if empty.
    pub fn is_empty(&self) -> bool {
        self.nodes.is_empty()
    }
    
    /// Get average edges per node.
    pub fn avg_edges(&self) -> f32 {
        if self.nodes.is_empty() {
            return 0.0;
        }
        
        let total_edges: usize = self.nodes.iter().map(|e| e.value().edges.len()).sum();
        total_edges as f32 / self.nodes.len() as f32
    }
    
    /// Get cache statistics.
    pub fn cache_stats(&self) -> (usize, usize) {
        (self.cache.len(), self.cache_stats.len())
    }
}

impl Default for SynthesisGraph {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    fn random_vector(dim: usize) -> Vector {
        let data: Vec<f32> = (0..dim)
            .map(|_| rand::random::<f32>() * 2.0 - 1.0)
            .collect();
        Vector::new(data, "test-model")
    }
    
    #[test]
    fn test_content_hash_consistency() {
        let v1 = Vector::new(vec![0.1, 0.2, 0.3], "test-model");
        let v2 = Vector::new(vec![0.1, 0.2, 0.3], "test-model");
        let v3 = Vector::new(vec![0.1, 0.2, 0.4], "test-model");
        
        let h1 = ContentHash::from_vector(&v1);
        let h2 = ContentHash::from_vector(&v2);
        let h3 = ContentHash::from_vector(&v3);
        
        assert_eq!(h1, h2);
        assert_ne!(h1, h3);
    }
    
    #[test]
    fn test_adaptive_tier_selection() {
        let graph = SynthesisGraph::new();
        
        // Small dataset - should use cold tier
        for _ in 0..50 {
            graph.insert(random_vector(128)).unwrap();
        }
        
        let query = random_vector(128);
        let results = graph.search(&query, 10).unwrap();
        
        // Small dataset uses cold tier (brute force)
        assert!(!results.is_empty());
        assert!(results[0].score > 0.0);
        
        // Add more vectors to cross threshold
        for _ in 0..2000 {
            graph.insert(random_vector(128)).unwrap();
        }
        
        let results2 = graph.search(&query, 10).unwrap();
        assert!(!results2.is_empty());
    }
    
    #[test]
    fn test_semantic_cache() {
        let graph = SynthesisGraph::new();
        
        // Insert vectors
        for _ in 0..100 {
            graph.insert(random_vector(128)).unwrap();
        }
        
        let query = random_vector(128);
        
        // First search - cold tier
        let results1 = graph.search(&query, 10).unwrap();
        assert!(!results1.is_empty());
        
        // Second search - should hit cache (hot tier)
        let results2 = graph.search(&query, 10).unwrap();
        assert!(!results2.is_empty());
        
        // Check cache was populated
        let (cache_size, _) = graph.cache_stats();
        assert!(cache_size > 0);
    }
    
    #[test]
    fn test_graph_connectivity() {
        let graph = SynthesisGraph::new_with_params(16, 100);
        
        for _ in 0..100 {
            graph.insert(random_vector(128)).unwrap();
        }
        
        let avg_edges = graph.avg_edges();
        assert!(avg_edges >= 8.0, "Graph should be well-connected");
    }
}
